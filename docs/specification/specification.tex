\documentclass[10pt,a4paper]{article}

\usepackage{fontspec}
\usepackage[top=1cm, bottom=2cm]{geometry}
\usepackage[backend=bibtex, sorting=none]{biblatex}
\usepackage{hyperref}
\usepackage{multirow}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont[Ligatures={Common},Numbers={Lining}]{Linux Libertine}
\author{Michal Staniaszek (michalst@kth.se)\\ Supervisor: John Folkesson}
\title{Master Thesis Specification}
\bibliography{../report/report.bib}
\begin{document}
\maketitle
\section{Problem Definition}
In this project, we will study techniques for searching point clouds generated
using RGB-D sensors such as the Microsoft Kinect. We are interested in being
able to extract query objects from large clouds, such as those generated by a
robot wandering around a corridor with offices off to the sides. An important
part of the task is that ``interesting'' objects are unknown. Thus, when
analysing the cloud it will be necessary to find objects without any prior
knowledge, which may require some form of segmentation. Since we don't want to
waste time defining walls and floors as objects, they would be removed in a
pre-processing step. To retrieve objects from the cloud it will be necessary to
describe objects in some way and add them to a database in so that it is
possible to distinguish between different objects. Whether this is done by
representing full objects or partial objects is still to be decided. The
specifics of query objects are variable, but they could be either a single
frame, from which the query object must be extracted, or a full cloud generated
by scanning the object in a controlled environment. The former would be the
ideal case but may complicate things. The query objects will probably be
extracted manually from clouds, but this process could also be automated using
segmentation. The final goal of the project is to have a system which can get
large clouds into a form where only objects are present, extract features of
some kind from the objects in the cloud, add them to a data structure, and then
query the structure for features similar to the ones generated from a query
object. The simplest way of demonstrating the system is to select some objects
present in the main cloud, and see how well the objects are retrieved when the
system is given multiple frames of the objects in question seen from different
viewpoints.
\section{Literature Study}
The literature study will use an initial pool of the papers cited in this
section, and will expand its scope to other relevant papers found in the
process. We will focus on 2D and 3D feature descriptors which can be used to
compress the point clouds into more manageable sizes, and to use later in object
queries \cite{saenko2011practical, lai2011scalable, wohlkinger2011shape,
  mueller2013recognition, bo2011hierarchical}. Segmentation methods will also be
studied in order to better understand the available techniques, but whether or
not the project will make use of segmentation is unknown. It may be possible to
combine 2D and 3D segmentation methods for better results
\cite{haris1998watershed, woo2002new, rabbani2006segmentation}. In addition, we
will look at methods for efficient storage and retrieval of feature descriptor
data \cite{nister2006scalable, philbin2007object}.

\section{Problem Solving Methods}
For segmentation, some sort of oversegmentation and recombination or clustering
methods such as k-means will be used. As mentioned, it may not be necessary to
do the segmentation directly. Plane extraction can be done using built in ROS
libraries which use RANSAC. It may be possible to use existing code for parts of
the project which are not the main focus in order to save time.
\section{Required Resources}
\begin{itemize}
\item Ubuntu 14.04 machine with ROS install
\item Dataset with large registered point cloud map, along with individual
  point clouds used to construct the map
\item Access to objects present in the map, so that multiple viewpoint clouds
  can be constructed to use in experiments
\item Access to Kinect/PrimeSense (maybe)
\end{itemize}
\section{Boundaries}
The project will be limited to finding query objects in the map. While it could
be possible to put the system onto a working robot, this will probably not be
necessary as the retrieval is the main focus of the project. 
\begin{table}
  \centering
  \begin{tabular}{c|l|c}
    Week \# & Start date &  Activity \\\hline
    1 & Jan 19 &\multirow{2}{*}{Gathering papers, developing understanding of
      project aims}\\
    2 & Jan 26 &\\\hline
    3 & Feb 2 &\multirow{4}{*}{Literature study (writing introduction and
      background sections of final report)}\\
    4 & Feb 9 &\\
    5 & Feb 16 &\\
    6 & Feb 23 &\\\hline
    7 & March 2 & Dataset acquisition, system setup, understanding existing code
    (if any)\\
    8 & March 9 & Extraction of objects from the full cloud, initial naive
    method implementation\\
    9 & March 16 & Finish naive method, do some experiments\\
    10 & March 23 & Write up naive method in report, start more complex method\\
    11 & March 30 & Continue with more complex method\\\hline
    12 & April 6 & complex method \\
    13 & April 13 & complex method \\
    14 & April 20 & complex method \\
    15 & April 27 & finish complex method, experiments \\
    16 & May 4 & Write up complex method\\\hline
    17 & May 11 & report and presentation, method tweaks/improvements (complete draft)\\
    18 & May 18 & report and presentation, method tweaks/improvements\\
    19 & May 25 & report and presentation, method tweaks/improvements\\
    20 & Jun 1 & report and presentation, method tweaks/improvements\\
  \end{tabular}
  \caption{Timetable for thesis work}
  \label{tab:time}
\end{table}
\printbibliography
\end{document}