\documentclass[11pt,a4paper]{kth-mag}

% \usepackage{fontspec}
\usepackage[backend=bibtex]{biblatex}
% \usepackage[top=1cm]{geometry}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{moreverb}
\usepackage[toc,page]{appendix}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
% \defaultfontfeatures{Mapping=tex-text}
% \setromanfont[Ligatures={Common},Numbers={Lining}]{Linux Libertine}

\bibliography{report.bib}

\author{Michal Staniaszek}
\title{A Comparative Study on the Effect of Interest Point Methods and
  Descriptors on Feature-Feature Matching for Object Query in Point Clouds}

\begin{document}
\maketitle
\begin{abstract}
  Brief description of the aim of the project, the data used, some of the
  techniques used, some experimental results.
\end{abstract}
\tableofcontents
\chapter{Introduction}
Having a large amount of data is in most cases a good thing. Data, in an
abstract sense, is the driving force behind the actions of every living thing,
and as such holds great power. However, in order to make use of data, it is
necessary to have some way of interacting with it in a useful way, and further
processing it. While technologies for storing and interacting with data have
been around for millennia, for the most part they were inconvenient and
cumbersome. Writing allowed practically lossless transfer of information between
generations, and is no doubt one of the most important inventions in the history
of humanity. That being said, books are quite limited, especially when one wants
to investigate a specific topic. Libraries are partial solutions to the problem,
but most libraries don't possess all books in existence, and while indexes exist
for a reason, it is still not easy to find what one is looking for.

With the internet and the immense amount of data available to its users, this
problem of finding what one is looking for has been compounded, and good ways of
getting around the problem have launched one of the most successful companies in
history. At first, listing all of the early internet to create a database was a
realistic proposition, and for some time this was a satisfactory solution.
However, as the number of accessible data on the internet grew, the system
became gradually more impractical. It could take minutes or even hours to get a
result for a query, and the trawling of content caused network slowdowns
\cite{firstsearch,archieabout,bowman1993research}. Subsequent work in the area
led to the development of search engines which were able to search for words in
pages, and various innovations led this to become the very effective way of
searching that we know today \cite{brin1998anatomy,pinkerton1994finding}.

While images have been on the internet since the early days, in recent years the
advent of affordable digital cameras and the ubiquity of mobile phone cameras
has led to hundreds of thousands of photographs being uploaded to the internet
every minute \cite{fbipo,photosminute}. At its most basic, image search utilises
the same techniques as text search, with information being extracted from
metadata like tags, descriptions and keywords \cite{jing2008pagerank}. More
recently, reverse image search has become more popular, allowing users to find
similar images to an example by extracting information from textures and trying
to find other images which contain similar information \cite{lew2006content}.
There is still much information present in images that cannot currently be
extracted and represented using image processing techniques, and this is an
active research area.

An emerging method of data storage that will need to be searchable in the near
future is 3D models and point clouds. 3D models have been used for a long time
in computer games, medical imaging, and animation. More recently, developments
in 3D printing have led to a growing number of websites which distribute models
to use for printing \cite{3dprintlist}. For many years these sorts of models
have been created using CAD programs, or in the case of object scanning,
expensive time-of-flight cameras. The release of the Microsoft Kinect in late
2010 marked a turning point in the realm of 3D image processing, creating an
affordable and effective method of gathering 3D data. Many research groups
quickly purchased the hardware, and much work has been done in the area since. A
3D equivalent of the popular 2D image processing library OpenCV quickly came
into existence for use with point clouds, as the data which comes from such 3D
sensors is known \cite{opencv, pcl}.

In this report, we will describe our approach the problem of retrieving from a
data set objects that are similar to some object that we have provided, which we
will call object query. In essence, we need to extract information from the data
set and the objects that we are interested in which describes their properties
in such a way that we can compare the descriptions to see if there are any
similarities that imply the presence of an object in the data set. While the
data set could be anything, in our particular case we have data from a project
which studies long-term robot operation in office environments. The data set
contains point clouds of a single office taken from the same position over a period
of approximately a month. Some objects in the data set have also been labelled,
so there is information about the positions of objects in the clouds.

While this project is not aiming to perform a specific task on an actual robotic
system, within this context there are applications to which an object query
system could be applied. Given a data set over long periods of time with clouds
taken at various locations, the system could be used to track the motion of
objects over time, and to provide information about where an object is likely to
be at a certain time. One potential application is to help people find objects
that have been misplaced.

The project will focus in particular on the implementation of a system which can
perform object query. It will evaluate a number of standard methods for
describing objects, and finding parts of objects that are particularly
discriminative. While it is possible to describe objects as a whole, we will
investigate the efficacy of using descriptions of small parts of the object to
retrieve matches from the data set.

Something about why why the problem is interesting as opposed to model matching
or somesuch

In chapter~\ref{chap:bg}, we explain some concepts that are important to
understand the work, provide background information on relevant parts of the
image processing literature, and attempt to introduce the reader to previous
work in similar areas. The specific techniques used in our system are described
in chapter~\ref{chap:devel}, and we also discuss our reasoning behind certain
steps of the process. Chapter~\ref{chap:impl} deals with the software, and how
the system was implemented, along with some explanation of how the system works.
Our experimental setup and the results of the experiments are described in
chapter~\ref{chap:exp}. We compare the quality of retrieval when different
methods are used, and also investigate the time taken by the system under
varying conditions. Finally, we summarise the system and our results in
chapter~\ref{chap:conc}.
\chapter{Background}
\label{chap:bg}
In this chapter we will introduce some key ideas relating to the project, and
papers which are related to what we are interested in doing. While some of the
techniques mentioned here are not directly used in the implementation of our
system, they can be useful for context, or to give examples of different ways of
approaching problems in this area. We discuss methods of finding interesting
regions in image and point cloud data, and how these regions can be represented
using descriptors, along with some methods for storing descriptors in ways that
make it easy and efficient to find similarities.

\section{Segmentation}
Segmentation encompasses techniques for splitting an image or a point cloud into
different parts, or grouping similar parts --- this is essentially two sides of
the same coin. In terms of images, segmentation might be used to try to find
background and foreground pixels, or for point clouds, to separate objects from
the surfaces on which they are resting. There are many different types of
methods in the area, which approach the problem from different starting points.

\begin{figure}
  \centerline{
    \subfloat[Superpixels size 64, 256 and 1024 computed using SLIC \cite{achanta2012slic}]{
      \includegraphics[height=0.3\textheight]{images/slic}
    }
    \subfloat[Supervoxel oversegmentation \cite{papon2013voxel}]{
      \includegraphics[height=0.3\textheight]{images/supervoxel_comb}
    }
  }
    \caption{Examples of 2D and 3D superpixel segmentations}
    \label{fig:meanshift}
\end{figure}

Superpixel clustering is the most common technique used for segmenting images.
The intent is to create regions in which all pixels have some sort of meaningful
relationship. Graph based algorithms treat pixels as nodes in a graph, where the
weights on edges between nodes are related to the similarity between the
connected pixels --- intensity, proximity and so on \cite{achanta2012slic}. The
most simple method is to use a threshold on the edge weights to create
superpixels. Fulkerson et al. use superpixel methods to identify object classes
in images \cite{fulkerson2009class}. An algorithm which applies the idea of
superpixels to point clouds to create supervoxels (3D pixels) has also been
developed \cite{papon2013voxel}.

Gradient ascent based algorithms iteratively improve clusters until some
criterion for convergence is reached \cite{achanta2012slic}. Popularised by
Comaniciu~\cite{comaniciu2002mean}, mean shift was first introduced by
Fukunaga~\cite{fukunaga1975estimation} in 1975, and rediscovered by
Cheng~\cite{cheng1995mean} in 1995. The technique finds stationary points in a
density estimate of the feature space, for example pixel RGB values, and uses
those points to define regions in the space by allocating pixels to them. One
common way of computing a density estimate is to place Gaussians at the location
of each pixel, and then to sum the values of all the Gaussians over the entire
space. Pixels which follow the gradient of the density to the same stationary
point are part of the same segment. An example can be seen in
Figure~\ref{fig:meanshift}.
\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{images/meanshift}
  \caption{Visualisation of mean shift \cite{comaniciu2002mean}. a) First two
    components of image pixels in LUV space. b) Decomposition found by running
    mean shift. c) Trajectories of mean shift over the density estimate.}
  \label{fig:meanshift}
\end{figure}


Random Sample Consensus (RANSAC) is a technique which uses shape models to find
ideal models in noisy data. Points in the data set are randomly sampled, and
used to construct a shape. For example, in the case of a line, two points are
sampled, and define the line. Distances from points in the data set to the model
defined by the randomly sampled points are then computed to find points which
are inliers to the model. This number is stored, and the process repeated a
number of times. At the end of the process, the model with the largest number of
inliers is returned \cite{fischler1981random}. RANSAC can be applied to
segmentation tasks by using it to find planes, cylinders, spheres and so on in
point clouds. In the case of planes this is particularly useful, as they are
usually not part of objects of interest, mostly making up walls, floors or
surfaces on which interesting objects rest. By removing the points corresponding
to these uninteresting surfaces, it should be possible to work only with parts
of clouds that contain objects of interest.

Several extensions to RANSAC have been proposed. Maximum Likelihood Estimation
Sample Consensus (MLESAC) chooses a solution that maximises the likelihood of
the model instead of just the number of inliers \cite{torr2000mlesac}.
M-estimator Sample Consensus (MSAC) uses a different cost function to the
original implementation, additionally scoring the inliers depending on how well
they fit the data \cite{torr2000mlesac}. The Progressive Sample Consensus
(PROSAC) uses prior information about the likelihood of input data being an
inlier or an outlier to limit the sampling pool and greatly reduce computation
cost \cite{chum2005matching}.


\section{Interest Points and Saliency}
Sipiran and Bustos extend the popular Harris detector \cite{harris1988combined}
to 3D \cite{sipiran2011harris}. Knopp et al. extend the SURF detector to 3D
\cite{knopp2010hough}.

Shilane and Funkhouser introduce a distinctiveness measure over classes of
meshed objects \cite{shilane2007distinctive}.

A multi-scale signature defined by the heat diffusion properties of an object
called the Heat Kernel Signature (HKS) \cite{sun2009concise} is used in
\cite{ovsjanikov2009shape} to retrieve shapes.

\cite{zhong2009intrinsic}

\cite{smith1997susan} extended to 3d by adding normal direction variation to
intensity variation

\section{Descriptors}
The problem of describing regions of an image in a compact and useful manner has
been studied for a long time in the computer vision community. For any given
point in an image, we would like to create a description which can be used to
represent the region around the point in some way. This descriptor, or feature,
can then be compared to other descriptors to see if there is some similarity. If
the similarity is within a given threshold, then we can assume that the points
represented by the two descriptors come from the same object, or represent the
same thing in both images. Thus, it is important to create features which are
distinct for different regions. In addition, since objects move around and can
be seen from different sides, or in different lights, an attractive property of
descriptors is to give similar results for the same region which has been
transformed in some way. In practice, this is quite difficult to achieve.
\subsection{2D}
While 2D descriptors are not directly usable on point clouds, the ideas that
they use to give effective results can be transferred over to use for 3D
description.

The Laplacian of Gaussians was introduced by Lindeberg, and uses derivatives
combined with some other techniques to select interest points.
\cite{lindeberg1998feature}. This paper also introduces the concept of automatic
scale selection for feature detection, which has played an important part in the
field since then. The scale of features can be investigated by blurring an image
using a Gaussian kernel --- higher standard deviation blurs the image more,
resulting in the removal of small scale features.

Even today the Scale Invariant Feature Transform (SIFT) is among the most
popular descriptors for 2D images. It is invariant to scale and rotation, and is
robust to some variation in affine distortion, viewpoint and illumination, and
is distinctive, allowing for correct matching of single features in large
databases. There are several stages of computation. Extrema are found in
different scales to find points invariant to scale and orientation. Keypoints
are selected at the extrema based on their stability. Image gradients at the
keypoint are used to define its orientation for future computations. The image
gradients are then transformed into a local descriptor vector with length 128
\cite{lowe2004distinctive}.

Mikolajczyk and Schmid~\cite{mikolajczyk2004scale} introduce the Harris-Laplace
detector which is an improvement on SIFT \cite{lowe2004distinctive} and the
Laplacian of Gaussians \cite{lindeberg1998feature} in the sense that it is able
to deal with affine transformations. They do not, however, introduce a new type
of descriptor to go with the point selection. 

Speeded-Up Robust Features (SURF) is a more recent descriptor which can be
computed and compared much faster than most other descriptors. It makes use of
integral images, which replace pixels in an image or image patch with a
cumulative sum of the pixel intensities over the rows and columns. This allows
for fast computation of pixel intensities in an area of the image. SURF takes
some ideas from SIFT, using the spatial distribution of gradients as a
descriptor, but integrates over the gradients instead of using individual
values, which makes it more robust to noise. The resulting descriptor is a 64
element vector, which means that it is also faster to compare than SIFT
\cite{bay2008speeded}.

\subsection{3D}
One early descriptor which remains popular is the spin image. The descriptor is
generated from a mesh model at oriented points with a surface normal. A plane
intersecting the normal with a certain width and height is rotated around the
normal, forming a cylinder. The plane is separated into bins. The bins
accumulate the number of points which pass through a certain bin during the
rotation. The resulting 2D image is the descriptor. By varying the width of the
plane the region which defines the descriptor can be modified. A small width
will give a local descriptor, while a large width will give a descriptor for the
whole image \cite{johnson1997spin,johnson1999using}. Figure~\ref{fig:spinimg}
shows a visualisation of how the image is generated.

\begin{figure}
  \centering
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin1}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin2}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin3}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin4}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin5}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin6}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin7}}
  \subfloat{\includegraphics[width=0.11\textwidth]{images/spin8}}
  \caption{Frames from construction of a spin image \cite{johnson1997spin}. The
    image plane spins around the oriented point normal and accumulates points.}
  \label{fig:spinimg}
\end{figure}

The Ensemble of Shape Functions (ESF) descriptor introduced in
\cite{wohlkinger2011ensemble} by Wohlkinger and Vincze combines the Shape
Distribution approach introduced by \cite{osada2002shape} along with some
extensions proposed in \cite{ip2002using}. It also makes use of their
voxel-based distance measure from \cite{wohlkinger2011shapedist}. Pairs or triples of
points are sampled from segmented partial clouds of objects, and histograms are
created by extracting information such as distance, angle, ratios, and whether
points are inside or outside (or a mix) of the model. See
Figure~\ref{fig:wohlESF}.

The Point Feature Histogram (PFH) was introduced by Rusu et al. in
\cite{rusu2008persistent}. It creates descriptors based on the angles between a
point on a surface and $k$ points close to it. The Fast Point Feature Histogram
(FPFH) improved the speed of computation, and allowed the use of the descriptor
in real time \cite{rusu2009fast}. The Viewpoint Feature Histogram (VFH) extended
the FPFH by adding viewpoint information to the histogram by computing
statistics of surface normals relative to the viewpoint \cite{rusu2010fast}. It
also improved the speed of the FPFH. The clustered version (CVFH) further
improved the viewpoint technique by mitigating the effect of missing parts and
extending it to facilitate estimation of the rotation of objects \cite{aldoma2011cad}.

Bo et al. develop the kernel descriptor initially created for RGB images for use
on depth images and point clouds. The kernels are used to describe size, shape
and edge features. Local features are combined to object-level features . Kernel
descriptors avoid the need to quantise attributes. Similarity is instead defined
by a match kernel \cite{bo2010kernel}, which improves recognition accuracy
\cite{bo2011depth}.

The point pair feature describes the relation between two oriented points on a
model. This means that it does not depend so much on the quality and resolution
of the model data. The model is described by grouping the point pair features of
the model, providing a global distribution of all the features on the model
surface \cite{drost2010model}.

\begin{figure}
  \centering
  \subfloat[3DSC \cite{frome2004recognizing}]{
    \includegraphics[width=0.24\textwidth]{images/3dsc}
    \label{fig:3dsc}
  }
  \subfloat[SHOT \cite{tombari2010unique}]{
    \includegraphics[width=0.24\textwidth]{images/shot}
    \label{fig:shot}
  }
  \subfloat[Context Shape \cite{shentu2008context}]{
    \includegraphics[width=0.24\textwidth]{images/contextshape}
    \label{fig:contextshape}
  }
  \subfloat[Integral Volume \cite{gelfand2005robust}]{
    \includegraphics[width=0.24\textwidth]{images/volint}
    \label{fig:volint}
  }
  \caption{Visualisation of spherical descriptors.}
  \label{fig:descexample}
\end{figure}

3D Shape Context (3DSC) is an extension of the original Shape Context descriptor
for 2D images \cite{belongie2002shape}. A sphere is placed at a point, and its
``top'' is oriented to match the direction of the normal at the point. Bins are
created within the sphere by equally spaced boundaries in the azimuth and
elevation, and logarithmically spaced boundaries in the radial dimension (Figure~\ref{fig:3dsc}). The
logarithmic spacing means that shape distortions far from the basis point have
less effect on the descriptor. Each bin accumulates a weighted count based on
the volume of the bin and local point density \cite{frome2004recognizing}. 3DSC
does not compute a local reference frame --- the vector of the azimuth is chosen
randomly, and subdivisions computed from that. This means that a number of
descriptors equal to the number of azimuth divisions need to be computed and
stored in order to compensate, and the matching process is complicated as a
result. The Unique Shape Context (USC) solves this problem by defining a local
reference frame and using the directions of that reference frame to subdivide
the sphere \cite{tombari2010uniquesc}.

The Signature of Histograms of Orientations (SHOT) descriptor improves on 3DSC
by taking inspiration from SIFT and making extensive use of histograms. The
sphere is split into 32 volumes: 8 azimuth regions, 2 elevation and 2 radial
(Figure~\ref{fig:shot}). A local histogram is computed in each of the regions,
using the angle between the normal of points and the feature point. The local
histograms are then combined to form the final descriptor
\cite{tombari2010unique}. The authors also extend the descriptor to include
colour (COLORSHOT) \cite{tombari2011combined}.

The Rotation Invariant Feature Transform (RIFT) is a generalisation of SIFT. Using
intensity values computed at each point from the RGB values, a gradient is
computed. Concentric rings are placed around the initial point, and a histogram
of the gradient orientations is created for points within each ring. The
orientation of the gradient is computed relative to the line from the central
point so that the descriptor is rotation invariant. The descriptor is 2D --- one
dimension is the distance, the other the gradient angle. The distance between
two descriptors is measured using the Earth Mover's Distance (EMD), which is a
measure of the distance between two probability distributions
\cite{lazebnik2005sparse}.

Multi-scale descriptors are useful as they can be used to characterise regions
of varying size. Cipriano et al. introduce such a descriptor for use on meshes
\cite{cipriano2009multi}. It captures the statistics of the shape of the
neighbourhood of a vertex by fitting a quadratic surface to it. Vertices in the
region are weighted based on distance from an initial vertex, and a plane is
constructed using a weighted average of the face normals. The parameters of the
quadratic are then used to find its principle curvatures, which make up the
descriptor.

Work in protein-protein docking also uses 3D descriptors to help with
simulations of an otherwise lengthy and complex process. The Surface Histogram
is introduced by Gu et al. \cite{gu2012surface}, and uses the local geometry
around two points with specific normals on the surface of a protein. A
coordinate system is defined by the two points and the line between them, and a
rectangular voxel grid is defined around the points. The grid is then marked in
locations where the surface crosses the grid, and a 2D image is constructed by
squashing the data onto one of the axes. The descriptor is designed to
immediately give a potential pose for the docking.

Another example of a shape descriptor from biology is the Context Shape
\cite{shentu2008context}. A sphere is centred on a point, and rays are projected
from this point to points evenly distributed on the surface of the sphere (Figure~\ref{fig:contextshape}). Each
of the rays is divided into segments, with a binary value associated with each
segment depending on whether the segment is inside or outside the protein. To
compare the descriptor, a rotation is applied to match the rays, and a volume of
overlap is computed based on matching bits in the rays.
 
\begin{figure}
  \centering
  \subfloat[]{
    \includegraphics[width=0.23\textwidth]{images/wohl_d2}
  }
  \subfloat[]{
    \includegraphics[width=0.23\textwidth]{images/wohl_onoff}
  }
  \subfloat[]{
    \includegraphics[width=0.23\textwidth]{images/wohl_ratio}
  }
  \subfloat[]{
    \includegraphics[width=0.23\textwidth]{images/wohl_a3}
  }
  \caption{Examples of the measures used to construct the Ensemble of Shape
    Functions histograms of \cite{wohlkinger2011ensemble}. a) Distance between
    points. b) Whether the points are on or off the model, or mixed. c) Ratio of
  line segments on and off the surface of the model. d) Angle between pairs of lines.}
  \label{fig:wohlESF}
\end{figure}

The splash descriptor was introduced by Stein et al. \cite{stein1992structural}.
A point on the surface with a given surface normal (the reference normal) is
chosen, and a slice around that with some geodesic radius (distance along the
surface) is computed. Points on the circle are selected using some angle step,
and the normal at that point is determined. A super splash is when this process
is repeated for several different radii. For each normal on the circle,
additional angles between it and a coordinate system centred on the reference
normal are computed. These angles and the angle around the circle are then
mapped into a 3D space, where polygonal approximation is made, connecting each
point with a straight line. Some additional computation is done to allow the
encoded polygons to act as a hash. Figure~\ref{fig:splash} shows part of the
formulation.

\begin{figure}
  \centering
  \subfloat[]{
    \includegraphics[width=0.32\textwidth]{images/splash}
  }
  \subfloat[]{
    \includegraphics[width=0.32\textwidth]{images/splash_normals}
  }
  \subfloat[]{
    \includegraphics[width=0.32\textwidth]{images/splash_angles}
  }
  \caption{Splash descriptor \cite{stein1992structural}. a) shows the splash and
    normals around it. b) and c) show how the additional angles are defined.
  }
  \label{fig:splash}
\end{figure}

Point Signatures are similar to the splash descriptor in the sense that they
both sample points on a circle \cite{chua1997point}. This descriptor again
selects a reference normal, and has a specific radius. This time, the radius
defines a sphere around the point. The intersection of the surface with the
sphere is a 3D space curve. The orientation of the curve is defined by fitting a
plane to it. The distances between the space curve and the fitted plane at
sampled points define the signature of the reference point. These signatures can
be compared by lining them up and checking whether the query falls within the
tolerance band of previous signatures. Figure~\ref{fig:pointsig} shows
signatures from various surfaces.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/pointsig}
  \caption{Examples of the point signature responses to different surfaces
    \cite{chua1997point}. $d$ is the distance from the reference vector to the
    space curve defined by the intersection of the surface with a sphere centred
    at $N$. Ref rotates about $N$.}
  \label{fig:pointsig}
\end{figure}

\subsubsection{Descriptors With Interest Point Extraction}
While many descriptors designed for 2D applications also select interest points
during an initial step in the process, the 3D descriptors that we have mentioned
above do not automatically find locations in the cloud which are good points at
which to compute descriptors.

The Normal Aligned Radial Feature (NARF) is an interest point extraction method
with a feature descriptor. A score for the image points is determined based on
the surface changes at the point, and information about borders. An interest
value is computed from this based on the score of the surrounding points.
Smoothing is applied, and non-maximum suppression is applied to find the final
interest points. To compute the descriptor, rays are projected over the range
image from the centre at certain intervals. The intensities of cells lying under
the ray are weighted based on their distance from the centre, and a normalised
weighted average of the pairwise difference of cells is used to define each
element of the descriptor vector, which has a length equal to the number of rays
\cite{steder2011point}. The method is an improvement on a previous paper by the
authors \cite{steder2009robust}. A problem with this method is that it uses
range images directly. Point clouds can be used to generate range images by
looking at them from different viewpoints, but this adds complexity to the
method.

The integral volume descriptor is interesting as it combines interest point
selection and description into one. The descriptor is defined as the volume of
the intersection of a sphere centred at a point on the surface of an object with
the inside of the object (Figure~\ref{fig:volint}). Interest points are selected
by histogramming the descriptor values, identifying bins with a number of points
less than a specified values, and selecting points from these bins. To ensure
features are properly spaced, points in a certain radius of already selected
points cannot be used. By modifying the radius of the sphere used to generate
the descriptor, interest points at different scales can be selected
\cite{gelfand2005robust}.

\section{Matching Point Clouds}
\subsection{Point Matching}
In \cite{chui2003new}, Chui and Rangarajan introduce an extension to ICP which
allows for non-rigid registration and improved robustness to outliers. In
contrast to ICP, their approach does not use the nearest-neighbour approach to
define correspondence. Instead, they use an alternating algorithm similar to
expectation maximisation. Annealing is used to prevent binary correspondences
when the algorithm is not yet close to the solution --- at the beginning there
is a large search range for correspondences, which gradually shrinks as the
temperature decreases.

\subsection{Model Matching}
\begin{figure}
  \centering
  \subfloat[Conformal factors. High value indicates high required deformation to
  sphere \cite{ben2008characterizing}.]{
    \includegraphics[width=0.48\textwidth]{images/conformal.png}
    \label{fig:conform}
  }
  \caption{Model matching approaches}
  \label{fig:modelmatch}
\end{figure}
In \cite{ben2008characterizing}, Chen et al. describe another approach to model
matching using conformal factors. This technique uses ideas from conformal
geometry, transforming the mesh of an object such that it has a uniform Gaussian
curvature. Information is stored about how much deformation is needed locally to
globally transform the object into a sphere --- this is the conformal factor.
The factor is based on a global computation on the whole mesh, as opposed to
per-vertex computations of the Gaussian curvature, which makes it much smoother
and appropriate for use in histograms. The histogram of a sample of the factors
is used as a descriptor, and is pose invariant, as seen in
Figure~\ref{fig:conform}. The authors say that it should be possible to use the
approach in partial model matching.

\section{Storing and Querying Descriptors}
There are several techniques for storing and querying descriptors, mostly based
on some form of tree. Recently, the k-d tree\cite{bentley1975multidimensional,
  friedman1977algorithm} has been used for efficient approximate matching with
either an error bound \cite{arya1998optimal}, where there is a bound placed on
the error between the true nearest neighbour and the one found, or a time bound
\cite{beis1997shape}, where the search is stopped after examining a certain
number of leaf nodes. Further improvements on the k-d tree are introduced in
\cite{silpa2008optimised}, where multiple randomised trees are used to optimise
the search. A priority search tree algorithm is introduced in
\cite{muja2014scalable}, which appears to be very effective. This may be the
same one as in \cite{muja2009fast}. The algorithm in the last two papers has
been integrated into PCL, which is useful.

A different approach to nearest neighbour search is the balltree, which uses
hyperspheres in a hierarchy to enclose points in the space
\cite{omohundro1989five}. Unlike the k-d tree, regions on the same level of the
tree are allowed to intersect, and do not need to partition the whole space,
which gives the balltree its representative power.

The vocabulary tree \cite{nister2006scalable} makes use of techniques from
document search to index images. Using $k$-means clustering, construction stage
creates a hierarchical quantisation of the image patch descriptors. In the query
phase, descriptors are compared to the cluster centres, and go down the tree
until a leaf is reached. The path through the tree is used as a scoring measure
to present retrieval results.

Philbin et al. \cite{philbin2007object} show that flat (single-level) $k$-means
clustering can be scaled to large vocabulary sizes if approximate nearest
neighbour methods are used. Early systems for image retrieval used a flat
clustering scheme, which could not scale to large vocabularies
\cite{sivic2003video}. The paper also introduces a re-ranking method which uses
spatial correspondences, which improves the retrieval quality.

Boiman et al. \cite{boiman2008defense} introduce the Naive Bayes Nearest
Neighbour (NBNN) classifier. It uses nearest neighbour distances in the space of
descriptors instead of images, computing ``image-to-class'' distances without
quantising the descriptors. In general, quantisation allows for dimensionality
reduction, at the expense of the discriminative power of descriptors. NBNN ``can
exploit the discriminative power of both (few) high and (many) low informative
descriptors''. The problem here is that the classes must be known beforehand,
and in our case we do not have that information. The local NBNN
\cite{mccann2012local} does not do the search based on classes. Instead, all the
descriptors are merged into a k-d tree on which approximate $k$-NN is run to
find descriptors in the local region of a query descriptor. A distance to
classes not present in the $k$-NN region is approximated by the distance to the
$k+1$th neighbour.

Funkhouser and Shilane present a method for querying a database of 3D objects
represented by local shape features \cite{funkhouser2006partial}. Partial
matches (correspondences) are stored in a priority queue sorted by geometric
deformation and the feature similarity. This means that only objects in the
database with a high probability of being a match need to be processed.

Some work has been done on optimising the retrieval of relevant images by
learning from user input \cite{rui2000optimizing}. When retrieved images are
presented, the user ranks them in terms of relevance, and this rank is then used
to improve the relevance of future searches.

\chapter{System Development}
\label{chap:devel}
In this chapter, we will describe the development of the object query system,
and explain the reasoning behind our choices. The specific methods that are used
will also be described in more detail, and examples of the output of different
parts of the system will be shown.

\section{Preprocessing}
The first step in the object query system is to perform some preprocessing on
the clouds in the data set --- while not strictly necessary, there are some
benefits to doing so, chief of which is a reduction in computation time. The
data set that we have consists of around 80 clouds of a single room, taken at
different times during different days of approximately a month of time. The
clouds are made up of a number of intermediate frames, which are registered into
a complete cloud. The robot used to collect the clouds takes several sweeps of
the room, changing the angle of the camera after each sweep. The clouds are
constructed using frames taken from a sweep where the camera is pointing
slightly below the horizontal. Examples of the raw clouds can be seen in
Figure~\ref{fig:orig}.

We also have a number of subsets of the raw cloud which represent objects in the
raw cloud. These annotation clouds are generally quite small, varying in size
from 134 points to 14,149. We also apply some preprocessing steps for these
clouds.

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{images/orig_side}
  \includegraphics[width=0.49\textwidth]{images/orig_top}
  \includegraphics[width=0.49\textwidth]{images/orig_diag_left}
  \includegraphics[width=0.49\textwidth]{images/orig_diag_right}
  \caption{Sample raw cloud viewed from several angles}
  \label{fig:orig}
\end{figure}

\begin{figure}
  \centering
  \subfloat[backpack1]{\includegraphics[width=0.30\textwidth]{images/annotations/original/backpack1_front_ut}}
  \subfloat[backpack2]{\includegraphics[width=0.30\textwidth]{images/annotations/original/backpack2_front_ut}}
  \subfloat[helmet]{\includegraphics[width=0.30\textwidth]{images/annotations/original/helmet_front_ut}}\\
  \subfloat[chair3]{\includegraphics[width=0.30\textwidth]{images/annotations/original/chair3_front_ut}}
  \subfloat[pillow3]{\includegraphics[width=0.30\textwidth]{images/annotations/original/pillow3_front_ut}}
  \subfloat[laptop1]{\includegraphics[width=0.30\textwidth]{images/annotations/original/laptop1_front_ut}}\\
  \subfloat[laptop2]{\includegraphics[width=0.30\textwidth]{images/annotations/original/laptop2_front_ut}}
  \subfloat[hanger\_jacket]{\includegraphics[width=0.30\textwidth]{images/annotations/original/hanger_jacket_front_ut}}
  \subfloat[trash\_bin]{\includegraphics[width=0.30\textwidth]{images/annotations/original/trash_bin_front_ut}}\\
  \subfloat[chair1]{\includegraphics[width=0.30\textwidth]{images/annotations/original/chair1_front_ut}}
  \subfloat[chair2]{\includegraphics[width=0.30\textwidth]{images/annotations/original/chair2_front_ut}}
  \caption{Selection of objects from the dataset.}
  \label{fig:objects_ut}
\end{figure}

\subsection{Downsampling}
In their merged forms, the clouds on average contain approximately 4,300,000
points for a room which is around 4m wide, 5.5m deep and 3m high. This number of
points does not actually provide us with much additional information, since the
intermediate frames all have the same resolution. As such, we can safely
downsample the cloud to get a more reasonable number of points.

To downsample, we make use of a voxel grid, which splits the 3D space in which
the cloud sits into smaller subspaces of equal size called voxels. The width,
height and depth of voxels in the space can be specified, but we are interested
in keeping all dimensions the same resolution, and so we specify the parameters
so that each voxel is a cube. At this stage, we would like to perform a simple
downsampling to reduce the number of points, but we wish to keep small details
in the cloud --- something in the realm of a 1cm resolution is ideal in this
case.

Downsampling with a 1cm resolution gives a reduction in size of the clouds of on
average 78\%, to approximately 950,000 points.
Figure~\ref{fig:downsample_effect} shows the effect of the downsampling. While
there is slight degradation of the textures, this is to some extent a visual
effect which is viewpoint dependent. Most of the structure in the cloud is
retained, which is key. This step is important, as it greatly affects the speed
of computation of subsequent steps in the system, but it is a trade off. If the
downsampling resolution is too low, then we lose a lot of information about the
surface structure of parts of the cloud, and this is likely to lead to worse
performance when trying to find matches. How tolerant we are to low resolution
also depends on the kinds of objects that we are interested in finding. If we do
not care about smaller objects, then even with a lower resolution the results
should still be satisfactory. However, a lower resolution likely means that it
will be necessary to look at larger regions of space in order to describe
points. We will investigate the effects of this in chapter~\ref{chap:exp}.

Show some of the objects which are made up of slices due to viewing angle -
downsampling can reduce this slicing effect.

\begin{figure}
  \centering
  \centerline{
    \subfloat[Sofa]{
      \includegraphics[width=0.60\textwidth]{images/sofa_og}
      \includegraphics[width=0.60\textwidth]{images/sofa_ds}
    }\\
  }
  \centerline{
    \subfloat[Desk 1]{
      \includegraphics[width=0.60\textwidth]{images/johan_og}
      \includegraphics[width=0.60\textwidth]{images/johan_ds}
    }\\
  }
  \centerline{
    \subfloat[Desk 2 side]{
      \includegraphics[width=0.60\textwidth]{images/nils_side_og}
      \includegraphics[width=0.60\textwidth]{images/nils_side_ds}
    }\\
  }
  \centerline{
    \subfloat[Desk 2 front]{
      \includegraphics[width=0.60\textwidth]{images/nils_front_og}
      \includegraphics[width=0.60\textwidth]{images/nils_front_ds}
    }\\
  }
  \caption{The effect of downsampling. The left column shows the original
    clouds, the right column clouds downsampled with voxel size of 1cm^3.}
  \label{fig:downsample_effect}
\end{figure}

\subsection{Transformation and Trimming}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{images/orig_transformed}
  \caption{Original cloud and the transformed cloud. The original cloud
    is on the right. The coordinate axis shows the global reference frame ---
    none of the axes are aligned for the original cloud, but the transformed
    cloud is well aligned with the $x$-$y$ plane.}
  \label{fig:orig_transformed}
\end{figure}

Once the cloud has been downsampled, there is a little more that needs to be
done in order to get the cloud into a convenient form. The raw data that we have
has clouds which have their origin at the position of the camera while the room
was being scanned. Our data is a subset of a larger dataset which contains
clouds of more than one room --- if we were to use the data without applying any
additional transformations, all the clouds would sit on top of each other at the
origin, whereas we would ideally like to have them in their true position
relative to the origin. The robot collecting data knows its position, so this
information is stored.

As mentioned before, each cloud is a combination of a number of intermediate
frames, each of which has corresponding information about the pose of the camera
when the frame was taken, which we can use to transform the complete cloud into
its actual position in space.

An added benefit of this transformation is that it allows us to remove the floor
and ceiling by using a simple thresholding filter on the $z$ axis, as the floor
of the cloud is now aligned with the $x$-$y$ plane of the global reference
frame, as opposed to being aligned with the cloud's rotated reference frame
(Figure~\ref{fig:orig_transformed}) The threshold for the ceiling can be
determined by measuring the ceiling height, and the floor is assumed to be a
$z=0$. We add a small offset to each of the values to ensure that the parts are
correctly removed even if there is some noise.

Although we would like the system to be as generic as possible, the particular
subset of clouds that we are using have a large number of points outside the
room which do not give any useful information. To this end, we also include
additional filters on the $x$ axis to remove these points.
Figure~\ref{fig:trimmed} shows the end result of this step.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{images/trimmed_side}
  \includegraphics[width=0.45\textwidth]{images/trimmed_top}
  \includegraphics[width=0.45\textwidth]{images/trimmed_diag_left}
  \includegraphics[width=0.45\textwidth]{images/trimmed_diag_right}
  \caption{Result of trimming step. Transformed cloud is blue, trimmed is green.}
  \label{fig:trimmed}
\end{figure}

\subsection{Plane Extraction}
Having extracted normals from the cloud, we come to what is the most costly
preprocessing step. Due to the structured nature of our dataset, the number of
planes present in the clouds is quite high. While the presence of planes can be
used to define surfaces and the like, in our system we are not interested in
using the planes for anything in particular, and as such removing them from the
cloud is good, because we remove a large portion of points in the clouds which
are not be parts of any object, speeding up computation time of subsequent
steps. An example of the result of this step can be seen in
Figure~\ref{fig:plane_extr}.

\begin{figure}
  \centering
  
  \caption{Example of the result of plane extraction.}
  \label{fig:plane_extr}
\end{figure}

Plane extraction is done by running RANSAC multiple times with a plane model. A
plane can be described by its general form equation
\begin{equation}
  \label{eq:1}
  ax+by+cz+d=0
\end{equation}
where the normal vector $\mathbf{n}$ is defined by the coefficients $a$, $b$ and
$c$. To get the model coefficients, RANSAC samples three points ($p_1,p_2$ and
$p_3$) from the input cloud. From these three points, the normal is computed
using the cross product \cite{planeeq}

\begin{equation}
  \label{eq:2}
  \mathbf{n}=(p_2-p_1)\times(p_3-p_1)
\end{equation}

Once the plane coefficients have been computed, we must find the inlier points of this
plane model, based on their distance to the plane. The perpendicular distance of
a point $p$ to a plane is
\begin{equation}
  \label{eq:3}
  D=\frac{\mathbf{n}\cdot p + d}{\mid \mathbf{n} \mid}
\end{equation}

A point is considered to be an inlier if $D<D_t$, where $D_t$ is some threshold
on the distance. The RANSAC algorithm repeats the point sampling $n$ times,
storing the plane coefficients and number of inliers. At the end of the process,
the best plane the one with the largest number of inliers.

While this simple formulation can work well, there can be issues where the
planes that are extracted are not actually planes, due to there being regions in
the cloud where there can be a large number of inliers, but no actual plane, as
seen in Figure~\ref{fig:planenormal}. This effect can be mitigated by including
a single additional step to the inlier check, which also looks at the angle
between the plane normal and the normal at the point, computed by
\begin{equation}
  \label{eq:5}
  \theta=\cos^{-1}(\mathbf{n}\cdot p)
\end{equation}

A point is then considered an inlier only if it passes the distance threshold
check and $\theta<\theta_t$, where $\theta_t$ is the threshold on the angle.
This simple addition gives much more consistent results.

The RANSAC implementation that we are working with uses only a single distance
computation
\begin{align}
  \label{eq:6}
  D_a&=w\theta + (1-w)D\\
  w&=(1-p_c)w_n
\end{align}
where $p_c$ is the curvature at the point $p$, and $w_n$ is a predefined weight
on the distance between the point and plane normals. $p_c\rightarrow 0$ on flat
surfaces, so in these regions the normal will have a higher influence on the
aggregate distance $D_a$, whereas in regions of high curvature the euclidean
distance will be more important. Inliers are points where $D_a<T$.

\begin{figure}
  \centering
  
  \caption{RANSAC with basic plane model and with plane-normal model.}
  \label{fig:planenormal}
\end{figure}

When extracting planes, we use several parameters in addition to the aggregate
distance threshold $T$ to tweak the behaviour. The main aim of the additional
parameters is to prevent planes which are too small from being extracted. We
set a hard limit on the total number of planes which can be extracted, and also
define a threshold on the minimum number of points $N_{\min}$ in a plane.
\begin{equation}
  \label{eq:7}
  N_{\min}=\max(\eta N_{\text{trim}},\, N_{\text{fixed}})
\end{equation}
where $\eta$ is a small positive positive value. Since we are dealing with large
clouds, a suitable range of values is $\left[0.02,0.05\right]$.
$N_{\text{trim}}$ is the number of points in the trimmed cloud.
$N_{\text{fixed}}$ is a fixed value. We choose the maximum of the two values to
ensure that fluctuations in the cloud size are compensated for.

\subsection{Normal Estimation}
In this step, normals are estimated for each point in the cloud. The normal at a
point is the vector which is perpendicular to the curvature of the surface at
that point. By estimating normals for clouds, we can get some more information
about the surface structure of the cloud. Normals are used in several parts of
the system, including by feature selection methods and features. As mentioned
above, they are also used in the plane extraction step to increase accuracy.

There are many ways of estimating normals, but the method we use is formulated
as a least squares plane fitting problem, which is used to estimate the normal
of the plane tangent to the surface at the point at which the normal is to be
computed \cite{RusuDoctoralDissertation}. The computation gives an ambiguous
result in terms of the sign of the normal. To correct for this, a viewpoint is
needed, which serves to define what sign is used. Perhaps the most important
thing to note is that the normal must be computed using points in a
neighbourhood; either within a certain radius, or the nearest $k$ points. The
neighbourhood determines the scale factor that results. A small neighbourhood
gives a small scale factor, and a large neighbourhood a large scale factor. A
large scale factor can be bad if the objects that one is trying to examine have
regions where the rate of change of surface curvature is high, such as at the
corners of tables. It results in the smearing of edges and the suppression of
fine detail \cite{RusuDoctoralDissertation}. Figure~\ref{fig:normal_corner}
shows an example of the effect of different neighbourhood sizes on the results.

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{images/normals_comb}
  \caption{Example of the smoothing effect of normal estimation radius. From
    bottom to top, 0.01, 0.025, 0.5, 0.2, 0.25, 0.5cm radius. Normals are
    indicated by orange lines. Note the tendency of normals with higher radius
    to tilt as they approach the corner. Normals on the top section are slightly
    skewed due to perspective.}
  \label{fig:normal_corner}
\end{figure}

Include figure with annotation objects with different normal radius

During preprocessing we compute two different sets of normals using different
settings for the radius. One set is for use with plane extraction, which has a
higher value for the radius, somewhat mitigating the effect of noise on the
normals, and resulting in less patchy extraction of planes
(Figure~\ref{fig:plane_normrad}).

\begin{figure}
  \centering
  
  \caption{Planes extracted with different settings for the normal radius}
  \label{fig:plane_normrad}
\end{figure}

\subsection{Annotations}
To ensure that the annotations for each cloud are in a similar form, we apply
the downsampling, transformation and normal extraction steps. The downsampling
step is particularly important here, as the raw annotations have issues with a
``slicing'' effect caused by the positioning of the camera when the frames were
taken, and its depth resolution (Figure~\ref{fig:slicing}). Using the raw clouds
to compute features would most likely result in worse performance as the surface
structure of the cloud is essentially a series of stacked planes, which would
lead to finding similar points anywhere where there is a flat surface.

\begin{figure}
  \centering
  \subfloat[Before downsampling]{
    \label{fig:slicing}
  }
  \subfloat[After downsampling]{}
  \caption{Annotations original and downsampled}
  \label{fig:annotation_ds}
\end{figure}
\section{Interest Point Selection}
Once the preprocessing step has been completed, we can move on to computing
features from the processed clouds. First, however, we need to choose the points
at which the features will be computed. The idea of interest point selection is
to choose points in the cloud which are better in some way than other points for
feature extraction --- which points these are depends on the method used. In
this section we will describe in some detail the methods that we use. All of the
methods used are part of the PCL keypoints library \cite{pcl_keypoints}.

\subsection{Uniform}
The first and most obvious method of selecting points for feature extraction is
not to try to select interesting points at all, but to simply spread points
uniformly over the space. With this method, one would expect to extract a larger
number of points than with targeted methods, and since the entire space is
covered, it is unlikely that there will be any omissions of points that are
interesting.

The problem with having a large number of points is that this results in more
features having to be computed and compared in later stages.

To compute the uniform points, we simply downsample the cloud once more. The
size of the voxels used determines the spread of the points over the space ---
the behaviour of this method is determined entirely by a single parameter.
\subsection{ISS}
Zhong~\cite{zhong2009intrinsic} introduces the intrinsic shape signature
interest point selection method as one of a series of steps in the computation
of the ISS descriptor introduced in the same paper.

The main component of this method is the scatter matrix, which is the covariance
matrix of points within a spherical region around a sample point. For a point
$p_i$, the $3\times 3$ scatter matrix is
\begin{equation}
  \label{eq:4}
  S(p_i)=\sum_{\mid p_j - p_i \mid < r_s}(p_j-p_i)(p_j-p_i)^T
\end{equation}
where $p_j$ is another point in the cloud. $r_s$ defines the saliency radius,
which limits the points which we consider to be in the neighbourhood of $p_i$.
Interest points are only extracted in regions where there are at least $n_{min}$
points in the neighbourhood of $p_i$.

Once $S$ is computed, its eigenvalues $\lambda^1_i, \lambda^2_i$ and
$\lambda^3_i$ (with decreasing magnitude) are extracted. The smallest eigenvalue
$\lambda^3_i$ can be used to measure the 3D point variations in the
neighbourhood of the point \cite{zhong2009intrinsic}. If it happens that two of
the eigenvalues computed are equal, the reference frame of the point can become
ambiguous, so limits are applied to the ratio of the eigenvalues such that
\begin{equation}
  \label{eq:8}
  \frac{\lambda^2_i}{\lambda^1_i}&< \gamma_{21},\, \frac{\lambda^3_i}{\lambda^2_i}&< \gamma_{32}
\end{equation}

With this formulation, it is likely that more points are considered interest
points than are not. To thin the interest points further, non-maximum
suppression is used. Essentially, this removes from the interest points any
point where the value of $\lambda^3_i$ at the point is not the maximum in the
neighbourhood of the point. This neighbourhood is defined by the radius $r_{n}$,
whose value is usually distinct from the value of $r_s$.
\subsection{SUSAN}
The SUSAN (Smallest Univalue Segment Assimilating Nucleus) detector is based on
an algorithm introduced for 2D feature detection by Smith~\cite{smith1997susan}.
We use an extension of this detector to 3D. The basis for the SUSAN principle
comes from the concept of each image point having an associated local area which
has intensity and normal direction values that are similar to it.

A spherical region called the mask is defined, with some radius $r_m$ which has
at its centre a point referred to as the nucleus. Looking at the points within
the spherical region, we compare their values of the normal direction and
intensity to the nucleus values. From this comparison, a region of space which
has similar values to the nucleus can be defined. This region is known as the
univalue segment assimilating nucleus or USAN. Figure~\ref{fig:susan_nucleus}
shows this principle in the 2D case. The USAN contains information about the
structure of the cloud in small region. Depending on the position of the nucleus
in the cloud, the volume of the USAN will vary. In regions where all points are
similar, the USAN is large, and it is small when the region has a large
variation in point intensity and normal direction. Based on this observation,
using the inverted USAN volume as a feature detector should result in the
selection of descriptive points --- hence the name \emph{Smallest} USAN.

To compute SUSAN keypoints, the following process is applied to each point $p_i$
in the cloud. First, all points $p_j$ in the neighbourhood defined by $r_m$ are
found. We then define the USAN and the centroid of the mask. In order to be
considered as part of the USAN, a point must fulfill the inequalities
\begin{align}
  \label{eq:9}
  \left|I_i-I_j\right|&\leq I_t\\
  1-\mathbf{n}_i\cdot\mathbf{n}_j&\leq \theta_t
\end{align}
where $I$ is the intensity of a point, and $\mathbf{n}$ is the normal, and $I_t$
and $\theta_t$ are user-defined thresholds on the intensity and angular
difference. The intensity is computed from RGB values using
\begin{align}
  \label{eq:10}
  I=\frac{r+g+b}{3}
\end{align}

We assume that each channel of the RGB value of a point has the same weight.

The centroid $C$ is computed using
\begin{equation}
  \label{eq:11}
  C=\frac{1}{\left|\text{USAN}\right|}\sum_{p_j \in \text{USAN}}p_j
\end{equation}

A geometric threshold is then applied to the USAN to ensure that the proportion
of points in the neighbourhood is not too large. This threshold is set to 50\%
of the neighbourhood.
\begin{figure}
  \centering
  \includegraphics[width=0.48\textwidth]{images/susan1}
  \includegraphics[width=0.48\textwidth]{images/susan}
  \caption{Concept of nucleus and mask in 2D SUSAN detector. USAN is the white
    region in the right image \cite{zhang2013susan}.}
  \label{fig:susan_nucleus}
\end{figure}
\subsection{SIFT}
\subsection{Harris}
\section{Feature Extraction}
Talk specifically about each of the features used and their good/bad points, and
how the features extraction is done (not particularly complex, probably)
\subsection{SHOT}
\subsection{SHOTCOLOR}
\subsection{USC}
\subsection{PFH}
\subsection{FPFH}
\subsection{PFHRGB}

\section{Object Query}
flann citations \cite{muja2009fast, muja2014scalable}

\chapter{Experimental Results}
\label{chap:exp}
Description of experiments and experimental results with different
configurations of the system.
\section{Segmentation}
\begin{table}
  \centering
  \centerline{
    \begin{tabular}{c|cccccc}
      &Original & Downsampled & Trimmed & Num planes & Points on planes & After preprocessing\\\hline
      Default&4344879\pm125893 & 964004\pm114379 & 810469\pm97833 & 7.10\pm1.48&456273\pm56508 & 354195\pm77925\\
      dt 0.02 & 4348571\pm124781 & 718345\pm246373 & 599503\pm208953 & 7.99\pm3.07&492154\pm152830 & 107348\pm64374\\
    \end{tabular}
  }
  \caption{Average number of points in various stages of preprocessing.}
  \label{tbl:preprocess_num}
\end{table}

\begin{table}
  \centering
  \centerline{
    \begin{tabular}{c|cccccc}
      &Load & Downsample & Trim & Normals & Planes & per plane time \\\hline
      Default & 1.91\pm0.58 & 1.08\pm0.31 & 20.12\pm13.56 & 30.95\pm10.88 & 221.62\pm130.12 & 31.36\pm16.09\\
      dt 0.02 & 2.45\pm1.25 & 1.18\pm0.17 & 0.16\pm0.06 & 21.65\pm13.36 & 235.50\pm118.06 & 29.08\pm7.50\\
    \end{tabular}
  }
  \caption{Average time taken for various stages of preprocessing.}
  \label{tbl:preprocess_time}
\end{table}
Apply different segmentation methods/parts of the segmentation method, and see
how they affect the overall system performance, as well as the time taken to
complete the segmentation.
\section{Feature Extraction}
Same as above, but for feature extraction. Try different features and see which
give best performance. Consider normal extraction cost for those features which
require it.
\section{Object Query}
This is evaluated in the previous two sections, but maybe there is more than one
method of finding the query object. Multi-index tree vs single index tree, for
example.


\chapter{Conclusion and Further Work}
\label{chap:conc}
Describe what the project was about, what was achieved, summarise the
experiments, describe what can be improved.

\begin{appendices}
  \chapter{Implementation}
  \label{chap:impl}
  Description of any parts of the implementation which are non standard or worth a
  mention. Perhaps talk about PCL and version control, as well as documentation
  and some other general software approaches to the system?
\end{appendices}
\printbibliography
\end{document}